import os
from dotenv import load_dotenv
import nltk

# ======================
# 1. Chuáº©n bá»‹ NLTK
# ======================
nltk.download("punkt")
try:
    nltk.download("punkt_tab")  # cáº§n cho NLTK >=3.8.1
except:
    pass

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import NLTKTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

from langchain.schema import Document

# Load PDF
loader = PyPDFLoader("ChuÌ›oÌ›ng 2 BieÌ‚Ìn, haÌ†Ì€ng vaÌ€ kieÌ‚Ì‰u duÌ›Ìƒ lieÌ£Ì‚u.pdf")
pages = loader.load()

# Gá»™p toÃ n bá»™ text thÃ nh 1 Document duy nháº¥t
full_text = "\n".join([p.page_content for p in pages])
full_doc = [Document(page_content=full_text)]

# ======================
# 3. Chia nhá» vÄƒn báº£n báº±ng NLTK
# ======================
text_splitter= NLTKTextSplitter(
    chunk_size=1600,       # Ä‘á»™ dÃ i tá»‘i Ä‘a má»—i chunk (sá»‘ kÃ½ tá»±)
    chunk_overlap=400,     # sá»‘ kÃ½ tá»± overlap giá»¯a 2 chunk
    separator="\n\n"       # kÃ½ tá»± tÃ¡ch Ä‘oáº¡n (máº·c Ä‘á»‹nh theo NLTK sentence tokenizer)
)
splitted_docs = []

for doc in full_doc:
    chunks = text_splitter.split_text(doc.page_content)
    for chunk in chunks:
        splitted_docs.append(
            {
                "page_content": chunk,
                "metadata": doc.metadata,  # giá»¯ metadata (trang sá»‘, v.v.)
            }
        )

print(f"âœ‚ï¸ Sau khi chia chunk: {len(splitted_docs)} Ä‘oáº¡n")

# ======================
# 4. Khá»Ÿi táº¡o Embeddings
# ======================
device = "cuda" if os.environ.get("USE_GPU", "1") == "1" else "cpu"
embeddings = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-large-instruct",
    model_kwargs={"device": device},
    encode_kwargs={"normalize_embeddings": True},
)

# ======================
# 5. Táº¡o vector store tá»« text
# ======================
vectorstore = FAISS.from_texts(
    [d["page_content"] for d in splitted_docs],
    embeddings,
    metadatas=[d["metadata"] for d in splitted_docs],
)

# Save to disk (create folder if not exists)
save_path = "vector_db2chunk_nltk"
os.makedirs(save_path, exist_ok=True)
vectorstore.save_local(save_path)



# # ======================
# # 6. Truy váº¥n thá»­
# # ======================
# query = "Äáº·t tÃªn trong java"
# retriever = vectorstore.as_retriever()
# results = retriever.get_relevant_documents(query)
#
# print("ğŸ” Káº¿t quáº£ truy váº¥n:")
# for i, d in enumerate(results, 1):
#     print(f"\n--- Káº¿t quáº£ {i} (Trang {d.metadata.get('page', 'N/A')}) ---")
#     print(d.page_content)
